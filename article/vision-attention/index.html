<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="记录生活与学习，半佛半火箭的研究僧，励志做一个牛哄哄的CV大佬">
    <meta name="keyword"  content="日常与学习, 计算机视觉, 人工智能">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <title>
        
          [计算机视觉]计算机视觉中注意力(Attention)机制原理及其模型发展和应用 - 王雪辉 | Blog
        
    </title>

    <link rel="canonical" href="https://wangxuehui.site/article/vision-attention/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS --> 
    <link rel="stylesheet" href="/css/beantech.min.css">

    <link rel="stylesheet" href="/css/donate.css">
    
    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css">

    <link rel="stylesheet" href="/css/widget.css">

    <link rel="stylesheet" href="/css/rocket.css">

    <link rel="stylesheet" href="/css/signature.css">

    <link rel="stylesheet" href="/css/toc.css">

    <!-- Custom Fonts -->
    <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>

    <!-- Baidu Tongji -->
    
    <script>
        // dynamic User by Hux
        var _baId = '4f2eccee1de0964665ba04749f9826e9';

        // Originial
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?" + _baId;
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
    </script>
    

    <!-- Baidu Sitemap -->
    <script>
        (function(){
            var bp = document.createElement('script');
            var curProtocol = window.location.protocol.split(':')[0];
            if (curProtocol === 'https') {
                bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
            }
            else {
                bp.src = 'http://push.zhanzhang.baidu.com/push.js';
            }
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(bp, s);
        })();
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Modified by Yu-Hsuan Yen author-->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            background-image: url('http://hexo.wangxuehui.site/default.jpg')
            /*post*/
        
    }
    
    #signature{
        background-image: url('http://hexo.wangxuehui.site/BeanTechSign-white.png');
    }
    
</style>

<header class="intro-header" >
    <!-- Signature author -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#Computer Vision" title="Computer Vision">Computer Vision</a>
                            
                        </div>
                        <h1>[计算机视觉]计算机视觉中注意力(Attention)机制原理及其模型发展和应用</h1>
                        <h2 class="subheading">在微信公众号上看到的一篇关于Attention机制在视觉领域应用的小白科普文。</h2>
                        <span class="meta">
                            转载自：[微信公众号]-有三AI(yanyousan_ai) on
                            2019-08-03
                        </span>
                     
                    </div>
                


                </div>
            </div>
        </div>
    </div>
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">王雪辉</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
                    
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        <li>
                            <a href="/about/">About</a>
                        </li>
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content-layout.ejs -->
    <!-- Modify by Yu-Hsuan Yen -->

<!-- Post Content archive -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container-->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <p>Attention机制在近几年来在图像，自然语言处理等领域中都取得了重要的突破，被证明有益于提高模型的性能。Attention机制本身也是符合人脑和人眼的感知机制，这次我们主要以计算机视觉领域为例，讲述Attention机制的原理，应用以及模型的发展。</p>
<h2 id="attention机制与显著图"><a href="#Attention机制与显著图" class="headerlink" title="Attention机制与显著图"></a><strong>Attention机制与显著图</strong></h2><h3 id="何为attention机制"><a href="#何为Attention机制" class="headerlink" title="何为Attention机制"></a><strong>何为Attention机制</strong></h3><p>所谓Attention机制，便是聚焦于局部信息的机制，比如图像中的某一个图像区域。随着任务的变化，注意力区域往往会发生变化。</p>
<p><img src="http://hexo.wangxuehui.site/vision-attention/what-is-attention.jpg" alt=""></p>
<p>面对上面这样的一张图，如果你只是从整体来看，只看到了很多人头，但是你拉近一个一个仔细看就了不得了，都是天才科学家(中间的爱大佬)。</p>
<p>图中除了人脸之外的信息其实都是无用的，也做不了什么任务，<strong>Attention机制便是要找到这些最有用的信息</strong>，可以想见最简单的场景就是从照片中检测人脸了。</p>
<h3 id="基于attention的显著目标检测"><a href="#基于Attention的显著目标检测" class="headerlink" title="基于Attention的显著目标检测"></a><strong>基于Attention的显著目标检测</strong></h3><p>和注意力机制相伴而生的一个任务便是显著目标检测，即<strong>Salient Object Detection</strong>。它的输入是一张图，输出是一张概率图，概率越大的地方，代表是图像中重要目标的概率越大，即人眼关注的重点，一个典型的显著图如下：</p>
<p><img src="http://hexo.wangxuehui.site/vision-attention/typical-salient-image.jpg" alt=""></p>
<p>右图就是左图的显著图，在头部位置概率最大，另外腿部，尾巴也有较大概率，这就是图中真正有用的信息。</p>
<p>显著目标检测需要一个数据集，而这样的数据集的收集便是通过追踪多个实验者的眼球在一定时间内的注意力方向进行平均得到，典型的步骤如下：</p>
<ol>
<li><p>让被测试者观察图。</p>
</li>
<li><p>用eye tracker记录眼睛的注意力位置。</p>
</li>
<li><p>对所有测试者的注意力位置使用高斯滤波进行综合。</p>
</li>
<li><p>结果以0～1的概率进行记录。</p>
</li>
</ol>
<p>于是就能得到下面这样的图，第二行是眼球追踪结果，第三行就是显著目标概率图。</p>
<p><img src="http://hexo.wangxuehui.site/vision-attention/original-tracker-salient.jpg" alt=""></p>
<p>上面讲述的都是空间上的注意力机制，即关注的是不同空间位置，而在CNN结构中，还有不同的特征通道，因此不同特征通道也有类似的原理，下面一起讲述。</p>
<h2 id="attention模型架构"><a href="#Attention模型架构" class="headerlink" title="Attention模型架构"></a><strong>Attention模型架构</strong></h2><p>注意力机制的本质就是定位到感兴趣的信息，抑制无用信息，结果通常都是以概率图或者概率特征向量的形式展示，从原理上来说，主要分为<strong>空间注意力模型，通道注意力模型，空间和通道混合注意力模型三种，这里不区分soft和hard attention。</strong></p>
<h3 id="空间注意力模型spatial-attention"><a href="#空间注意力模型-spatial-attention" class="headerlink" title="空间注意力模型(spatial attention)"></a><strong>空间注意力模型(spatial attention)</strong></h3><p>不是图像中所有的区域对任务的贡献都是同样重要的，只有任务相关的区域才是需要关心的，比如分类任务的主体，空间注意力模型就是寻找网络中最重要的部位进行处理。</p>
<p>这里给大家介绍两个具有代表性的模型，第一个就是Google DeepMind提出的STN网络(Spatial Transformer Network[1])。它通过学习输入的形变，从而完成适合任务的预处理操作，是一种基于空间的Attention模型，网络结构如下：</p>
<p><img src="http://hexo.wangxuehui.site/vision-attention/stn.jpg" alt=""></p>
<p>这里的Localization Net用于生成仿射变换系数，输入是<script type="math/tex">C \times H \times W</script>维的图像，输出是一个空间变换系数，它的大小根据要学习的变换类型而定，如果是仿射变换，则是一个6维向量。</p>
<p>这样的一个网络要完成的效果如下图：</p>
<p><img src="http://hexo.wangxuehui.site/vision-attention/stn-result.jpg" alt=""></p>
<p>即定位到目标的位置，然后进行旋转等操作，使得输入样本更加容易学习。这是一种一步调整的解决方案，当然还有很多迭代调整的方案，感兴趣可以去本文作者：<strong>有三</strong><a href="https://wx.zsxq.com/mweb/views/joingroup/join_group.html?group_id=822451554112" target="_blank" rel="noopener">知识星球</a>中阅读。</p>
<p>相比于Spatial Transformer Networks 一步完成目标的定位和仿射变换调整，Dynamic Capacity Networks[2]则采用了两个子网络，分别是低性能的子网络(coarse model)和高性能的子网络(fine model)。低性能的子网络(coarse model)用于对全图进行处理，定位感兴趣区域，如下图中的操作$f_c$。高性能的子网络(fine model)则对感兴趣区域进行精细化处理，如下图的操作$f_f$。两者共同使用，可以获得更低的计算代价和更高的精度。</p>
<p><img src="http://hexo.wangxuehui.site/vision-attention/dcn.jpg" alt=""></p>
<p>由于在大部分情况下我们感兴趣的区域只是图像中的一小部分，因此空间注意力的本质就是定位目标并进行一些变换或者获取权重。</p>
<h3 id="通道注意力机制"><a href="#通道注意力机制" class="headerlink" title="通道注意力机制"></a><strong>通道注意力机制</strong></h3><p>对于输入2维图像的CNN来说，一个维度是图像的尺度空间，即长宽，另一个维度就是通道，因此基于通道的Attention也是很常用的机制。</p>
<p>SENet(Sequeeze and Excitation Net)[3]是2017届ImageNet分类比赛的冠军网络，本质上是一个基于通道的Attention模型，它通过建模各个特征通道的重要程度，然后针对不同的任务增强或者抑制不同的通道，原理图如下:</p>
<p><img src="http://hexo.wangxuehui.site/vision-attention/SENet.jpg" alt=""></p>
<p>在正常的卷积操作后分出了一个旁路分支，首先进行Squeeze操作(即图中$F_{sq}(\cdot)$，它将空间维度进行特征压缩，即每个二维的特征图变成一个实数，相当于具有全局感受野的池化操作，特征通道数不变。</p>
<p>然后是Excitation操作(即图中的$F_{ex}(\cdot)$)，它通过参数$w$为每个特征通道生成权重，$w$被学习用来显式地建模特征通道间的相关性。在文章中，使用了一个2层bottleneck结构(先降维再升维)的全连接层+Sigmoid函数来实现。</p>
<p>得到了每一个特征通道的权重之后，就将该权重应用于原来的每个特征通道，基于特定的任务，就可以学习到不同通道的重要性。</p>
<p>将其机制应用于若干基准模型，在增加少量计算量的情况下，获得了更明显的性能提升。作为一种通用的设计思想，它可以被用于任何现有网络，具有较强的实践意义。而后SKNet[4]等方法将这样的通道加权的思想和Inception中的多分支网络结构进行结合，也实现了性能的提升。</p>
<p>通道注意力机制的本质，在于建模了各个特征之间的重要性，对于不同的任务可以根据输入进行特征分配，简单而有效。</p>
<h3 id="空间和通道注意力机制的融合"><a href="#空间和通道注意力机制的融合" class="headerlink" title="空间和通道注意力机制的融合"></a><strong>空间和通道注意力机制的融合</strong></h3><p>前述的Dynamic Capacity Network是从空间维度进行Attention，SENet是从通道维度进行Attention，自然也可以同时使用空间Attention和通道Attention机制。</p>
<p>CBAM(Convolutional Block Attention Module)[5]是其中的代表性网络，结构如下：</p>
<p><img src="http://hexo.wangxuehui.site/vision-attention/CBAM.jpg" alt=""></p>
<p>通道方向的Attention建模的是特征的重要性，结构如下：</p>
<p><img src="http://hexo.wangxuehui.site/vision-attention/channel-attention.jpg" alt=""></p>
<p>同时使用最大pooling和均值pooling算法，然后经过几个MLP层获得变换结果，最后分别应用于两个通道，使用sigmoid函数得到通道的attention结果。</p>
<p>空间方向的Attention建模的是空间位置的重要性，结构如下：</p>
<p><img src="http://hexo.wangxuehui.site/vision-attention/spacial-attention.jpg" alt=""></p>
<p>首先将通道本身进行降维，分别获取最大池化和均值池化结果，然后拼接成一个特征图，再使用一个卷积层进行学习。  </p>
<p>这两种机制，分别学习了通道的重要性和空间的重要性，还可以很容易地嵌入到任何已知的框架中。</p>
<p>除此之外，还有很多的注意力机制相关的研究，比如<strong>残差注意力机制，多尺度注意力机制，递归注意力机制等。</strong></p>
<h2 id="attention机制典型应用场景"><a href="#Attention机制典型应用场景" class="headerlink" title="Attention机制典型应用场景"></a><strong>Attention机制典型应用场景</strong></h2><p>从原理上来说，注意力机制在所有的计算机视觉任务中都能提升模型性能，但是有两类场景尤其受益。</p>
<h3 id="细粒度分类"><a href="#细粒度分类" class="headerlink" title="细粒度分类"></a><strong>细粒度分类</strong></h3><p>关于细粒度分类的基础内容，可以回顾作者公众号的<a href="http://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&amp;mid=2649034832&amp;idx=2&amp;sn=5d66b2d6eec1b05bd6cc95bbd8b0dae1&amp;chksm=8712ae2db065273bc71cffc7decba2f7459f88a60d528f60379d8eb1a99d0b50bd59102ac211&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">往期文章</a>。</p>
<p><img src="http://hexo.wangxuehui.site/vision-attention/fine-image.jpg" alt=""></p>
<p>我们知道细粒度分类任务中真正的难题在于如何定位到真正对任务有用的局部区域，如上示意图中的鸟的头部。Attention机制恰巧原理上非常合适，文[1],[6]中都使用了注意力机制，对模型的提升效果很明显。</p>
<h3 id="显著目标检测缩略图生成自动构图"><a href="#显著目标检测-缩略图生成-自动构图" class="headerlink" title="显著目标检测/缩略图生成/自动构图"></a><strong>显著目标检测/缩略图生成/自动构图</strong></h3><p>我们又回到了开头，没错，Attention的本质就是重要/显著区域定位，所以在目标检测领域是非常有用的。</p>
<p><img src="http://hexo.wangxuehui.site/vision-attention/object-detection.jpg" alt=""></p>
<p><img src="http://hexo.wangxuehui.site/vision-attention/object-detection-2.jpg" alt=""></p>
<p>上图展示了几个显著目标检测的结果，可以看出对于有显著目标的图，概率图非常聚焦于目标主体，在网络中添加注意力机制模块，可以进一步提升这一类任务的模型。</p>
<p>除此之外，在视频分析，看图说话等任务中也比较重要。</p>
<p>参考文献：</p>
<blockquote>
<p>[1] Jaderberg M, Simonyan K, Zisserman A. Spatial transformer networks[C]//Advances in neural information processing systems. 2015: 2017-2025.</p>
<p>[2] Almahairi A, Ballas N, Cooijmans T, et al. Dynamic capacity networks[C]//International Conference on Machine Learning. 2016: 2549-2558.</p>
<p>[3] Hu J, Shen L, Sun G. Squeeze-and-excitation networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 7132-7141.</p>
<p>[4] Li X, Wang W, Hu X, et al. Selective Kernel Networks[J]. 2019.</p>
<p>[5] Woo S, Park J, Lee J Y, et al. Cbam: Convolutional block attention module[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 3-19.</p>
<p>[6] Fu J, Zheng H, Mei T. Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4438-4446.</p>
</blockquote>

                

                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                    
                        <li class="next">
                            <a href="/article/PointCloud-architecture/" data-toggle="tooltip" data-placement="top" title="[阅读总结]PointCloud Basic Architecture总结梳理">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                <br>

                <!--打赏-->
                
                    <div class="reward">
                        <div class="reward-button">赏 <span class="reward-code"> 
                            <span class="alipay-code"> <img class="alipay-img" src="http://hexo.wangxuehui.site/alipay.JPG"><b>支付宝打赏</b></span> 
                            <span class="wechat-code"> <img class="wechat-img" src="http://hexo.wangxuehui.site/wechat.JPG"><b>微信打赏</b> </span>
                            </span></div>
                        <p class="reward-notice">赞赏一下</p>
                    </div>
                
                <!--打赏-->

                <br>
                <!--分享-->
                
                    <div class="social-share"  data-wechat-qrcode-helper="" align="center"></div>
                    <!--  css & js -->
                    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
                    <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
                
                <!--分享-->
                <br>                       
                
                <!-- require APlayer -->
                

                <!-- duoshuo Share start -->
                
                <!-- 多说 Share end-->

                <!-- 多说评论框 start -->
                
                <!-- 多说评论框 end -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->

                
                    <!-- disqus 评论框 start -->
                    <div class="comment">
                        <div id="lv-container" data-id="city" data-uid="MTAyMC80MzM4OS8xOTkzMA=="></div>
                    </div>
                    <!-- disqus 评论框 end -->
                

            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

  
    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#attention机制与显著图"><span class="toc-nav-number">1.</span> <span class="toc-nav-text"><a href="#Attention&#x673A;&#x5236;&#x4E0E;&#x663E;&#x8457;&#x56FE;" class="headerlink" title="Attention&#x673A;&#x5236;&#x4E0E;&#x663E;&#x8457;&#x56FE;"></a><strong>Attention&#x673A;&#x5236;&#x4E0E;&#x663E;&#x8457;&#x56FE;</strong></span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#何为attention机制"><span class="toc-nav-number">1.1.</span> <span class="toc-nav-text"><a href="#&#x4F55;&#x4E3A;Attention&#x673A;&#x5236;" class="headerlink" title="&#x4F55;&#x4E3A;Attention&#x673A;&#x5236;"></a><strong>&#x4F55;&#x4E3A;Attention&#x673A;&#x5236;</strong></span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#基于attention的显著目标检测"><span class="toc-nav-number">1.2.</span> <span class="toc-nav-text"><a href="#&#x57FA;&#x4E8E;Attention&#x7684;&#x663E;&#x8457;&#x76EE;&#x6807;&#x68C0;&#x6D4B;" class="headerlink" title="&#x57FA;&#x4E8E;Attention&#x7684;&#x663E;&#x8457;&#x76EE;&#x6807;&#x68C0;&#x6D4B;"></a><strong>&#x57FA;&#x4E8E;Attention&#x7684;&#x663E;&#x8457;&#x76EE;&#x6807;&#x68C0;&#x6D4B;</strong></span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#attention模型架构"><span class="toc-nav-number">2.</span> <span class="toc-nav-text"><a href="#Attention&#x6A21;&#x578B;&#x67B6;&#x6784;" class="headerlink" title="Attention&#x6A21;&#x578B;&#x67B6;&#x6784;"></a><strong>Attention&#x6A21;&#x578B;&#x67B6;&#x6784;</strong></span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#空间注意力模型spatial-attention"><span class="toc-nav-number">2.1.</span> <span class="toc-nav-text"><a href="#&#x7A7A;&#x95F4;&#x6CE8;&#x610F;&#x529B;&#x6A21;&#x578B;-spatial-attention" class="headerlink" title="&#x7A7A;&#x95F4;&#x6CE8;&#x610F;&#x529B;&#x6A21;&#x578B;(spatial attention)"></a><strong>&#x7A7A;&#x95F4;&#x6CE8;&#x610F;&#x529B;&#x6A21;&#x578B;(spatial attention)</strong></span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#通道注意力机制"><span class="toc-nav-number">2.2.</span> <span class="toc-nav-text"><a href="#&#x901A;&#x9053;&#x6CE8;&#x610F;&#x529B;&#x673A;&#x5236;" class="headerlink" title="&#x901A;&#x9053;&#x6CE8;&#x610F;&#x529B;&#x673A;&#x5236;"></a><strong>&#x901A;&#x9053;&#x6CE8;&#x610F;&#x529B;&#x673A;&#x5236;</strong></span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#空间和通道注意力机制的融合"><span class="toc-nav-number">2.3.</span> <span class="toc-nav-text"><a href="#&#x7A7A;&#x95F4;&#x548C;&#x901A;&#x9053;&#x6CE8;&#x610F;&#x529B;&#x673A;&#x5236;&#x7684;&#x878D;&#x5408;" class="headerlink" title="&#x7A7A;&#x95F4;&#x548C;&#x901A;&#x9053;&#x6CE8;&#x610F;&#x529B;&#x673A;&#x5236;&#x7684;&#x878D;&#x5408;"></a><strong>&#x7A7A;&#x95F4;&#x548C;&#x901A;&#x9053;&#x6CE8;&#x610F;&#x529B;&#x673A;&#x5236;&#x7684;&#x878D;&#x5408;</strong></span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#attention机制典型应用场景"><span class="toc-nav-number">3.</span> <span class="toc-nav-text"><a href="#Attention&#x673A;&#x5236;&#x5178;&#x578B;&#x5E94;&#x7528;&#x573A;&#x666F;" class="headerlink" title="Attention&#x673A;&#x5236;&#x5178;&#x578B;&#x5E94;&#x7528;&#x573A;&#x666F;"></a><strong>Attention&#x673A;&#x5236;&#x5178;&#x578B;&#x5E94;&#x7528;&#x573A;&#x666F;</strong></span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#细粒度分类"><span class="toc-nav-number">3.1.</span> <span class="toc-nav-text"><a href="#&#x7EC6;&#x7C92;&#x5EA6;&#x5206;&#x7C7B;" class="headerlink" title="&#x7EC6;&#x7C92;&#x5EA6;&#x5206;&#x7C7B;"></a><strong>&#x7EC6;&#x7C92;&#x5EA6;&#x5206;&#x7C7B;</strong></span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#显著目标检测缩略图生成自动构图"><span class="toc-nav-number">3.2.</span> <span class="toc-nav-text"><a href="#&#x663E;&#x8457;&#x76EE;&#x6807;&#x68C0;&#x6D4B;-&#x7F29;&#x7565;&#x56FE;&#x751F;&#x6210;-&#x81EA;&#x52A8;&#x6784;&#x56FE;" class="headerlink" title="&#x663E;&#x8457;&#x76EE;&#x6807;&#x68C0;&#x6D4B;/&#x7F29;&#x7565;&#x56FE;&#x751F;&#x6210;/&#x81EA;&#x52A8;&#x6784;&#x56FE;"></a><strong>&#x663E;&#x8457;&#x76EE;&#x6807;&#x68C0;&#x6D4B;/&#x7F29;&#x7565;&#x56FE;&#x751F;&#x6210;/&#x81EA;&#x52A8;&#x6784;&#x56FE;</strong></span></a></li></ol></li></ol>
        
        </div>
      </aside>
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#Computer Vision" title="Computer Vision">Computer Vision</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="http://blog.csdn.net/huwh_" target="_blank">CSDN Blog 胡伟煌</a></li>
                    
                        <li><a href="http://www.carlib.net" target="_blank">无人系统研究所</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>






    <!-- 来必力City版公共JS代码 start (一个网页只需插入一次) -->
    <script type="text/javascript">
       (function(d, s) {
           var j, e = d.getElementsByTagName(s)[0];
    
           if (typeof LivereTower === 'function') { return; }
    
           j = d.createElement(s);
           j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
           j.async = true;
    
           e.parentNode.insertBefore(j, e);
       })(document, 'script');
    </script>
    <noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
    <!-- 来必力City版 公共JS代码 end -->



<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                
                
                    <li>
                        <a target="_blank" href="https://www.zhihu.com/people/sduuuu">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa  fa-stack-1x fa-inverse">知</i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank" href="http://weibo.com/5364025234">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                
                    <li>
                        <a target="_blank"  href="https://github.com/wxxxxxxh">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Xuehui Wang 2019 
                    <br>
                    Theme by <a href="http://www.huweihuang.com">胡伟煌</a> 
                    <span style="display: inline-block; margin: 0 5px;">
                        <i class="fa fa-heart"></i>
                    </span> 
                    re-Ported by <a href="http://wangxuehui.site">王雪辉</a> | 
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="91px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=huweihuang&repo=hexo-theme-huweihuang&type=star&count=true" >
                    </iframe>
                    <br>
                    <span class="post-meta-item-icon">
                        <i class="fa fa-eye"></i>
                    </span>
                    本站总访问量 
                    <span id="busuanzi_value_site_pv">
                        <i class="fa fa-spinner fa-spin"></i>
                    </span> 
                    次，
                    总访问人数
                    <span id="busuanzi_value_site_uv">
                        <i class="fa fa-spinner fa-spin"></i>
                    </span> 
                    人
                    <br>
                    鲁ICP备18007290号 || 鲁公网安备 37100202000552号

                </p>
                
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("https://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="https://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://wangxuehui.site/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>



<!-- busuanzi -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"> </script>







	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>
<!-- Image to hack wechat -->
<img src="https://wangxuehui.site/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!--<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

</html>
